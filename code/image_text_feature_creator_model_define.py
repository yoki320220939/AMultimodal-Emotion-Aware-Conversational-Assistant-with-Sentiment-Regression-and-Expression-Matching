##æ„å»ºæ–‡å­—å’Œå›¾åƒç‰¹å¾é¢„æµ‹æƒ…æ„Ÿå‘é‡ï¼ˆé€šè¿‡å¤šæ¨¡æ€é¢„æµ‹æƒ…æ„Ÿï¼‰

import os
import json
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel
from PIL import Image
from tqdm import tqdm
import torch.optim as optim

# === æ¨¡å‹å®šä¹‰ ===
class MultimodalRegressor(nn.Module):
    def __init__(self, text_encoder, image_encoder, hidden_size=512, output_dim=2):
        super().__init__()
        self.text_encoder = text_encoder
        self.image_encoder = image_encoder

        self.fusion = nn.Sequential(
            nn.Linear(512 + 768, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
        )

        self.residual = nn.Linear(512 + 768, hidden_size)  # æ®‹å·®åˆ†æ”¯

        self.regressor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size // 2, output_dim)
        )

    def forward(self, input_ids, attention_mask, pixel_values):
        text_feat = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]
        image_feat = self.image_encoder.get_image_features(pixel_values=pixel_values)

        fused = torch.cat((text_feat, image_feat), dim=1)
        x = self.fusion(fused) + self.residual(fused)  # æ®‹å·®è¿æ¥
        output = self.regressor(x)
        return output

# === æ•°æ®é›†å®šä¹‰ ===
class MemeDataset(Dataset):
    def __init__(self, data, clip_processor, tokenizer, max_length=128):
        self.data = data
        self.clip_processor = clip_processor
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        image = Image.open(item["image_path"]).convert("RGB")
        text = item["text"]
        label = torch.tensor(item["logits"], dtype=torch.float)

        image_inputs = self.clip_processor(images=image, return_tensors="pt")
        pixel_values = image_inputs["pixel_values"].squeeze(0)

        text_inputs = self.tokenizer(text, padding="max_length", truncation=True,
                                     max_length=self.max_length, return_tensors="pt")
        input_ids = text_inputs["input_ids"].squeeze(0)
        attention_mask = text_inputs["attention_mask"].squeeze(0)

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "pixel_values": pixel_values,
            "label": label
        }

# === è®­ç»ƒå‡½æ•° ===
def train(model, dataloader, optimizer, device):
    model.train()
    total_loss = 0
    criterion = nn.MSELoss()

    for batch in tqdm(dataloader, desc="è®­ç»ƒä¸­"):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        pixel_values = batch["pixel_values"].to(device)
        labels = batch["label"].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask, pixel_values)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(dataloader)

# === éªŒè¯å‡½æ•° ===
def evaluate(model, dataloader, device):
    model.eval()
    total_loss = 0
    criterion = nn.MSELoss()

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="éªŒè¯ä¸­"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            pixel_values = batch["pixel_values"].to(device)
            labels = batch["label"].to(device)

            outputs = model(input_ids, attention_mask, pixel_values)
            loss = criterion(outputs, labels)
            total_loss += loss.item()

    return total_loss / len(dataloader)

# === ä¸»ç¨‹åºå…¥å£ ===
def main():
    MODEL_DIR = os.path.join(os.path.dirname(__file__), "models/bert-base-chinese")
    CLIP_DIR = os.path.join(os.path.dirname(__file__), "models/clip-vit-base-patch32")

    tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)
    text_encoder = BertModel.from_pretrained(MODEL_DIR)
    clip_model = CLIPModel.from_pretrained(CLIP_DIR)
    clip_processor = CLIPProcessor.from_pretrained(CLIP_DIR)

    model = MultimodalRegressor(text_encoder, clip_model, output_dim=2).cuda()

    with open("train_data.json", "r", encoding="utf-8") as f:
        train_data = json.load(f)
    with open("val_data.json", "r", encoding="utf-8") as f:
        val_data = json.load(f)

    train_dataset = MemeDataset(train_data, clip_processor, tokenizer)
    val_dataset = MemeDataset(val_data, clip_processor, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64)

    optimizer = optim.AdamW(model.parameters(), lr=2e-5)

    best_val_loss = float("inf")
    patience = 5
    patience_counter = 0

    for epoch in range(300):
        print(f"\n===== Epoch {epoch + 1} =====")
        train_loss = train(model, train_loader, optimizer, device="cuda")
        val_loss = evaluate(model, val_loader, device="cuda")

        print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f} | éªŒè¯æŸå¤±: {val_loss:.4f}")

        if val_loss < best_val_loss:
            print("ğŸ”„ éªŒè¯é›†æŸå¤±ä¸‹é™ï¼Œä¿å­˜æ¨¡å‹ã€‚")
            best_val_loss = val_loss
            patience_counter = 0
            torch.save(model.state_dict(), "best_model.pt")
        else:
            patience_counter += 1
            print(f"âš ï¸ éªŒè¯é›†æŸå¤±æ— æå‡ï¼ˆ{patience_counter}/{patience}ï¼‰")

        if patience_counter >= patience:
            print("â¹ï¸ æ—©åœè§¦å‘ï¼Œè®­ç»ƒç»“æŸã€‚")
            break

if __name__ == "__main__":
    main()
