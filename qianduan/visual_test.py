import streamlit as st
from PIL import Image
import os
from datetime import datetime
import json
import torch
import easyocr
import cv2
import numpy as np
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter

from sklearn.metrics.pairwise import cosine_similarity

# æ–‡æœ¬å¤„ç†ç›¸å…³
from transformers import (
    BertTokenizer, 
    BertModel,
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    MarianTokenizer, 
    MarianMTModel,
    BlipProcessor, 
    BlipForConditionalGeneration,
    CLIPProcessor, 
    CLIPModel
)

# æœºå™¨å­¦ä¹ ç›¸å…³
from sklearn.metrics.pairwise import cosine_similarity
from torch import nn
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim

##### æœ‰ä¸€ä¸ªæˆ‘ä»¬æ‡’å¾—ä¿®çš„bug,æ— æ³•é€šè¿‡å›¾ç‰‡åŒ¹é…è¾“å‡ºé‡å¤çš„å›¾ç‰‡
class MultimodalRegressor(nn.Module):
    def __init__(self, text_encoder, image_encoder, hidden_size=512, output_dim=2):
        super().__init__()
        self.text_encoder = text_encoder
        self.image_encoder = image_encoder
        
        self.fusion = nn.Sequential(
            nn.Linear(512 + 768, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
        )

        self.residual = nn.Linear(512 + 768, hidden_size)
        self.regressor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size // 2, output_dim)
        )

    def forward(self, input_ids, attention_mask, pixel_values):
        # Ensure inputs are on the same device as the model
        device = next(self.parameters()).device
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)
        pixel_values = pixel_values.to(device)
        
        text_feat = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]
        image_feat = self.image_encoder.get_image_features(pixel_values=pixel_values)

        fused = torch.cat((text_feat, image_feat), dim=1)
        x = self.fusion(fused) + self.residual(fused)
        output = self.regressor(x)
        return output
    
# class MultimodalRegressor(nn.Module):
#     def __init__(self, text_encoder, image_encoder, hidden_size=512, output_dim=2):
#         super().__init__()
#         self.text_encoder = text_encoder
#         self.image_encoder = image_encoder

#         self.fusion = nn.Sequential(
#             nn.Linear(512 + 768, hidden_size),
#             nn.ReLU(),
#             nn.Dropout(0.3),
#             nn.Linear(hidden_size, hidden_size),
#             nn.ReLU(),
#             nn.Dropout(0.3),
#             nn.Linear(hidden_size, hidden_size),
#             nn.ReLU(),
#             nn.Dropout(0.3),
#         )

#         self.residual = nn.Linear(512 + 768, hidden_size)  # æ®‹å·®åˆ†æ”¯

#         self.regressor = nn.Sequential(
#             nn.Linear(hidden_size, hidden_size // 2),
#             nn.ReLU(),
#             nn.Dropout(0.2),
#             nn.Linear(hidden_size // 2, output_dim)
#         )

    def forward(self, input_ids, attention_mask, pixel_values):
        text_feat = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]
        image_feat = self.image_encoder.get_image_features(pixel_values=pixel_values)

        fused = torch.cat((text_feat, image_feat), dim=1)
        x = self.fusion(fused) + self.residual(fused)  # æ®‹å·®è¿æ¥
        output = self.regressor(x)
        return output

# 1. è¡¨æƒ…åŒ…åŒ¹é…å™¨å®ç°
class MemeMatcher:
    def __init__(self, data_path):
        """åˆå§‹åŒ–è¡¨æƒ…åŒ…åŒ¹é…å™¨"""
        with open(data_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        # åˆå§‹åŒ–æƒ…æ„Ÿåˆ†ææ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained("./code/models/Erlangshen-Roberta-110M-Sentiment")
        self.model = AutoModelForSequenceClassification.from_pretrained("./code/models/Erlangshen-Roberta-110M-Sentiment")
        self.model.eval()
    
    def bleu_match(self, query, top_n=5):
        """åŸºäºBLEUåˆ†æ•°çš„å†…å®¹åŒ¹é…"""
        scores = []
        for item in self.data:
            score = self._calculate_bleu(query, item['content'])
            scores.append((item, score))
        
        # æŒ‰åˆ†æ•°é™åºæ’åº
        scores.sort(key=lambda x: x[1], reverse=True)
        return [x[0] for x in scores[:top_n]]
    
    def emotion_match(self, query, top_n=5):
        """åŸºäºæƒ…æ„Ÿå‘é‡çš„åŒ¹é…"""
        query_logits = self._get_emotion_features(query)
        
        similarities = []
        for item in self.data:
            sim = cosine_similarity([query_logits], [item['logits']])[0][0]
            similarities.append((item, sim))
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        return [x[0] for x in similarities[:top_n]]
    
    def _get_emotion_features(self, text):
        """è·å–æ–‡æœ¬æƒ…æ„Ÿç‰¹å¾"""
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            outputs = self.model(**inputs)
            return outputs.logits.squeeze().tolist()
    
    def _calculate_bleu(self, candidate, reference, max_n=4):
        """è®¡ç®—BLEUåˆ†æ•°"""
        def ngram_precision(cand, ref, n):
            cand_ngrams = [tuple(cand[i:i+n]) for i in range(len(cand)-n+1)]
            ref_ngrams = [tuple(ref[i:i+n]) for i in range(len(ref)-n+1)]
            
            cand_count = Counter(cand_ngrams)
            ref_count = Counter(ref_ngrams)
            
            overlap = sum(min(cand_count[ngram], ref_count[ngram]) for ngram in cand_count)
            return overlap / len(cand_ngrams) if cand_ngrams else 0
        
        precisions = [ngram_precision(candidate, reference, n) for n in range(1, max_n+1)]
        p_avg = np.mean(precisions)
        
        # ç®€æ´æƒ©ç½š
        brevity_penalty = 1 if len(candidate) > len(reference) else np.exp(1 - len(reference)/len(candidate))
        
        return brevity_penalty * np.exp(np.sum(np.log(np.maximum(p_avg, 1e-10))))

# åˆå§‹åŒ–åŒ¹é…å™¨ï¼ˆç¼“å­˜ä»¥æé«˜æ€§èƒ½ï¼‰
@st.cache_resource
def load_matcher():
    return MemeMatcher("./code/labeled_data_logits.json")

# å¢å¼ºçš„æ¶ˆæ¯ä¿å­˜å‡½æ•°
def save_message(sender, text=None, image=None, image_path=None):
    """ä¿å­˜æ¶ˆæ¯åˆ°æœ¬åœ°ï¼ˆæ”¯æŒç›´æ¥å›¾ç‰‡å’Œå›¾ç‰‡è·¯å¾„ï¼‰"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # å¤„ç†ç›´æ¥ä¸Šä¼ çš„å›¾ç‰‡
    if image:
        image_path = f"chat_messages/{timestamp}_{sender}.png"
        image.save(image_path)
    
    # è‡ªåŠ¨å¡«å……é»˜è®¤æ–‡æœ¬
    if (image or image_path) and (text is None or not text.strip()):
        text = "åˆ†äº«äº†ä¸€å¼ å›¾ç‰‡"
    
    with open("chat_messages/chat_history.txt", "a", encoding="utf-8") as f:
        f.write(f"{timestamp}|{sender}|{text or ''}|{image_path or ''}\n")

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå›¾æ ‡
st.set_page_config(
    page_title="GuGuçš„åŒäººèŠå¤©å®¤",
    page_icon="ğŸ’¬",
    layout="centered"
)

# åˆ›å»ºä¿å­˜æ¶ˆæ¯çš„ç›®å½•
if not os.path.exists("chat_messages"):
    os.makedirs("chat_messages")

# åŠ è½½æ‰€æœ‰æ¨¡å‹ï¼ˆæ”¾åœ¨å…¨å±€ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
@st.cache_resource
def load_models():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    # æ–‡æœ¬åŒ¹é…æ¨¡å‹
    text_model_path = "./code/models/bert-base-chinese"
    text_tokenizer = BertTokenizer.from_pretrained(text_model_path)
    text_model = BertModel.from_pretrained(text_model_path).to(device)
    text_model.eval()

    # æƒ…æ„Ÿåˆ†ææ¨¡å‹
    emotion_model_path = "./code/models/Erlangshen-Roberta-110M-Sentiment"
    emotion_tokenizer = AutoTokenizer.from_pretrained(emotion_model_path)
    emotion_model = AutoModelForSequenceClassification.from_pretrained(emotion_model_path).to(device)
    emotion_model.eval()
    
    # ç¿»è¯‘æ¨¡å‹
    translation_model_path = "./code/models/opus-mt-en-zh_model"
    translation_tokenizer = MarianTokenizer.from_pretrained(translation_model_path)
    translation_model = MarianMTModel.from_pretrained(translation_model_path).to(device)
    
    # å›¾ç‰‡æè¿°æ¨¡å‹
    image_caption_path = "./code/models/blip-image-captioning-base"
    image_processor = BlipProcessor.from_pretrained(image_caption_path)
    image_model = BlipForConditionalGeneration.from_pretrained(image_caption_path).to(device)
    
    # å¤šæ¨¡æ€æ¨¡å‹ç»„ä»¶
    clip_model_path = "./code/models/clip-vit-base-patch32"
    clip_processor = CLIPProcessor.from_pretrained(clip_model_path)
    clip_model = CLIPModel.from_pretrained(clip_model_path).to(device)
    clip_model.eval()
    
    # å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ¨¡å‹
    multimodal_model_path = "./code/models/multimodal-regressor"
    multimodal_model = MultimodalRegressor(
        text_encoder=text_model,  # é‡ç”¨å·²æœ‰çš„BERTæ¨¡å‹
        image_encoder=clip_model,
        output_dim=2
    ).to(device)
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    if os.path.exists(os.path.join(multimodal_model_path, "best_model.pt")):
        multimodal_model.load_state_dict(
            torch.load(
                os.path.join(multimodal_model_path, "best_model.pt"),
                map_location="cuda" if torch.cuda.is_available() else "cpu"
            )
        )
    multimodal_model.eval()

    # åŠ è½½æ•°æ®é›†
    with open("./code/quotes_with_features.json", "r", encoding="utf-8") as f:
        dataset = json.load(f)
    
    return {
        "text_tokenizer": text_tokenizer,
        "text_model": text_model,
        "emotion_tokenizer": emotion_tokenizer,
        "emotion_model": emotion_model,
        "translation_tokenizer": translation_tokenizer,
        "translation_model": translation_model,
        "image_processor": image_processor,
        "image_model": image_model,
        "clip_processor": clip_processor,
        "clip_model": clip_model,
        "multimodal_model": multimodal_model,
        "dataset": dataset,
        "device": "cuda" if torch.cuda.is_available() else "cpu"
    }

models = load_models()

def analyze_multimodal_sentiment(text=None, image=None):
    """
    å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ
    å‚æ•°:
        text: æ–‡æœ¬å†…å®¹ (å¯é€‰)
        image: PIL.Imageå¯¹è±¡ (å¯é€‰)
    è¿”å›:
        (negative, positive) æƒ…æ„Ÿlogits
    """
    device = models["device"]
    model = models["multimodal_model"].to(device)
    
    # å¤„ç†æ–‡æœ¬è¾“å…¥
    if text:
        text_inputs = models["text_tokenizer"](
            text, return_tensors="pt", 
            truncation=True, padding=True, 
            max_length=128
        )
        input_ids = text_inputs["input_ids"].to(device)
        attention_mask = text_inputs["attention_mask"].to(device)
    else:
        # å¦‚æœæ²¡æœ‰æ–‡æœ¬ï¼Œä½¿ç”¨ç©ºæ–‡æœ¬
        input_ids = torch.tensor([[0]]).to(device)  # [CLS] token
        attention_mask = torch.tensor([[1]]).to(device)
    
    # å¤„ç†å›¾åƒè¾“å…¥
    if image:
        try:
            image = image.convert("RGB")
            image_inputs = models["clip_processor"](
                images=image, return_tensors="pt"
            )
            pixel_values = image_inputs["pixel_values"].to(device)
        except Exception as e:
            st.error(f"å›¾åƒå¤„ç†å¤±è´¥: {str(e)}")
            pixel_values = torch.zeros(1, 3, 224, 224).to(device)  
    else:
        pixel_values = torch.zeros(1, 3, 224, 224).to(device) 
    # è¿è¡Œæ¨¡å‹
    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            pixel_values=pixel_values
        )
    
    negative, positive = outputs.squeeze().tolist()
    return negative, positive

@st.cache_resource
def load_ocr_reader():
    return easyocr.Reader(['ch_sim', 'en'])  # ä¸­è‹±æ–‡è¯†åˆ«

# ä¿®æ”¹ ocr_extract_text() å‡½æ•°
def ocr_extract_text(image):
    reader = load_ocr_reader()  # ä½¿ç”¨ç¼“å­˜çš„reader
    img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    results = reader.readtext(img_cv)
    return "\n".join([text for (_, text, _) in results])

# æƒ…æ„Ÿåˆ†æåŠŸèƒ½
def analyze_sentiment(texts):
    """
    åˆ†æå¤šæ¡æ–‡æœ¬çš„æƒ…æ„Ÿ
    è¿”å›: (negative_logits, positive_logits)
    """
    negative_logits = []
    positive_logits = []
    
    for text in texts:
        inputs = models["emotion_tokenizer"](text, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            outputs = models["emotion_model"](**inputs)
            logits = outputs.logits.squeeze().tolist()
            negative_logits.append(logits[0])  # è´Ÿé¢æƒ…æ„Ÿlogits
            positive_logits.append(logits[1])  # æ­£é¢æƒ…æ„Ÿlogits
    
    return negative_logits, positive_logits

def plot_sentiment_curve(text_ids, negative_logits, positive_logits):
    """
    ç»˜åˆ¶æƒ…æ„Ÿæ›²çº¿å›¾
    :param text_ids: æ–‡æœ¬IDåˆ—è¡¨
    :param negative_logits: è´Ÿé¢æƒ…æ„Ÿlogitsåˆ—è¡¨
    :param positive_logits: æ­£é¢æƒ…æ„Ÿlogitsåˆ—è¡¨
    """
    fig, ax = plt.subplots(figsize=(10, 5))
    
    # ç»˜åˆ¶æ›²çº¿
    ax.plot(text_ids, negative_logits, 'b-', label='Negative emotions', linewidth=2)
    ax.plot(text_ids, positive_logits, 'r-', label='Positive emotion', linewidth=2)
    
    # å¡«å……æ›²çº¿ä¹‹é—´çš„åŒºåŸŸ
    ax.fill_between(text_ids, negative_logits, positive_logits, 
                    where=np.array(positive_logits)>=np.array(negative_logits), 
                    facecolor='red', alpha=0.1)
    ax.fill_between(text_ids, negative_logits, positive_logits, 
                    where=np.array(positive_logits)<np.array(negative_logits), 
                    facecolor='blue', alpha=0.1)
    
    # è®¾ç½®å›¾è¡¨å±æ€§
    ax.set_title('GuGu sentiment analysis curve', pad=20)
    ax.set_ylabel('Emotional intensity')
    ax.legend(loc='upper right')
    
    # éšè—æ¨ªåæ ‡æ ‡ç­¾
    ax.set_xticks([])
    
    # æ·»åŠ ç½‘æ ¼çº¿
    ax.grid(True, linestyle='--', alpha=0.6)
    
    plt.tight_layout()
    return fig

# BLEUåŒ¹é…åŠŸèƒ½
epsilon = 1e-10

def ngram_precision(candidate, reference, n):
    candidate_ngrams = [tuple(candidate[i:i+n]) for i in range(len(candidate)-n+1)]
    reference_ngrams = [tuple(reference[i:i+n]) for i in range(len(reference)-n+1)]
    
    candidate_ngrams_count = Counter(candidate_ngrams)
    reference_ngrams_count = Counter(reference_ngrams)

    overlap = sum(min(candidate_ngrams_count[ngram], reference_ngrams_count[ngram]) for ngram in candidate_ngrams_count)
    precision = overlap / len(candidate_ngrams) if candidate_ngrams else 0
    return precision

def calculate_bleu(candidate, reference, max_n=4):
    precisions = [ngram_precision(candidate, reference, n) for n in range(1, max_n+1)]
    p_avg = np.mean(precisions)
    
    candidate_length = len(candidate)
    reference_length = len(reference)
    
    if candidate_length > reference_length:
        brevity_penalty = 1
    else:
        brevity_penalty = np.exp(1 - reference_length / candidate_length) if candidate_length > 0 else 0
    
    bleu_score = brevity_penalty * np.exp(np.sum(np.log(np.maximum(p_avg, epsilon))))
    return bleu_score

def find_similar_by_bleu(input_sentence, top_n=3):
    dataset_texts = [item["quote"] for item in models["dataset"]]
    
    bleu_scores = []
    for quote in dataset_texts:
        bleu_score = calculate_bleu(input_sentence, quote, max_n=4)
        bleu_scores.append(bleu_score)
    
    top_indices = np.argsort(bleu_scores)[-top_n:][::-1]
    results = []
    for idx in top_indices:
        results.append({
            "text": dataset_texts[idx],
            "score": bleu_scores[idx],  # BLEUåˆ†æ•°
            "similarity": bleu_scores[idx]  # ä¹Ÿä½œä¸ºç›¸ä¼¼åº¦å­˜å‚¨
        })
    
    return results

# ç¿»è¯‘åŠŸèƒ½
def translate_text(text):
    device = models["device"]
    inputs = models["translation_tokenizer"]([text], return_tensors="pt", padding=True).to(device)
    translated = models["translation_model"].generate(**inputs)
    translated_text = models["translation_tokenizer"].decode(translated[0], skip_special_tokens=True)
    return translated_text

# å›¾ç‰‡æè¿°åŠŸèƒ½
def generate_image_description(image):
    device = models["device"] 
    try:
        # å°†å›¾ç‰‡è½¬æ¢ä¸ºRGBæ ¼å¼
        image = image.convert("RGB")
        # ç”Ÿæˆæè¿°
        inputs = models["image_processor"](images=image, return_tensors="pt").to(device)
        models["image_model"].to(device)
        with torch.no_grad():
            out = models["image_model"].generate(**inputs).to(device)
        
        description = models["image_processor"].decode(out[0], skip_special_tokens=True)
        
        # å°†è‹±æ–‡æè¿°ç¿»è¯‘æˆä¸­æ–‡
        chinese_description = translate_text(description)
        
        return {
            "english": description,
            "chinese": chinese_description
        }
    except Exception as e:
        st.error(f"ç”Ÿæˆå›¾ç‰‡æè¿°æ—¶å‡ºé”™: {str(e)}")
        return None

# æå–æ–‡æœ¬ç‰¹å¾ï¼ˆBERTï¼‰
def get_text_features(sentence):
    device = models["device"]  # è·å–æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
    inputs = models["text_tokenizer"](
        sentence, 
        return_tensors="pt", 
        truncation=True, 
        padding=True, 
        max_length=512
    ).to(device)  # å°†è¾“å…¥å¼ é‡ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
    
    with torch.no_grad():
        outputs = models["text_model"](**inputs)
        sentence_embedding = outputs.last_hidden_state[:, 0, :].squeeze().tolist()
    return sentence_embedding

# æå–æƒ…æ„Ÿç‰¹å¾
def get_emotion_features(sentence):
    device = models["device"]
    inputs = models["emotion_tokenizer"](
        sentence, 
        return_tensors="pt", 
        truncation=True, 
        padding=True, 
        max_length=512
    ).to(device)
    
    with torch.no_grad():
        outputs = models["emotion_model"](**inputs)
        logits = outputs.logits.squeeze().tolist()
    return logits

# æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„å¤šä¸ªå¥å­
def find_similar_sentences(input_sentence, top_n=3):
    dataset_features = [item["features"] for item in models["dataset"]]
    dataset_texts = [item["quote"] for item in models["dataset"]]

    input_text_features = get_text_features(input_sentence)
    input_emotion_features = get_emotion_features(input_sentence)

    text_similarities = []
    for features in dataset_features:
        text_features = features[:len(input_text_features)]
        similarity = cosine_similarity([input_text_features], [text_features])[0][0]
        text_similarities.append(similarity)

    emotion_similarities = []
    for features in dataset_features:
        emotion_features = features[len(input_text_features):]
        similarity = cosine_similarity([input_emotion_features], [emotion_features])[0][0]
        emotion_similarities.append(similarity)

    text_weight = 0.3
    emotion_weight = 0.7

    combined_similarities = [
        text_weight * text_similarity + emotion_weight * emotion_similarity
        for text_similarity, emotion_similarity in zip(text_similarities, emotion_similarities)
    ]

    top_indices = np.argsort(combined_similarities)[-top_n:][::-1]
    results = []
    for idx in top_indices:
        results.append({
            "text": dataset_texts[idx],
            "similarity": combined_similarities[idx],  # ç›¸ä¼¼åº¦
            "score": combined_similarities[idx]  # ä¹Ÿä½œä¸ºåˆ†æ•°å­˜å‚¨
        })
    
    return results

def save_message(sender, text=None, image=None):
    """ä¿å­˜æ¶ˆæ¯åˆ°æœ¬åœ°ï¼ˆå…¼å®¹å›¾ç‰‡å¯¹è±¡å’Œå›¾ç‰‡è·¯å¾„ï¼‰
    å‚æ•°:
        sender: å‘é€è€…åç§°
        text: æ¶ˆæ¯æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
        image: å¯ä»¥æ˜¯ä»¥ä¸‹ä¸¤ç§å½¢å¼ä¹‹ä¸€:
               - PIL.Image å¯¹è±¡ï¼ˆç›´æ¥ä¸Šä¼ çš„å›¾ç‰‡ï¼‰
               - str å›¾ç‰‡è·¯å¾„ï¼ˆè¡¨æƒ…åŒ…è·¯å¾„ï¼‰
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    image_path = None
    if isinstance(image, str):  # å¤„ç†å›¾ç‰‡è·¯å¾„
        if os.path.exists(image):
            image_path = image
        else:
            st.error(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image}")
            return
    elif image is not None:  # å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡å¯¹è±¡
        try:
            image_path = f"chat_messages/{timestamp}_{sender}.png"
            image.save(image_path)
        except Exception as e:
            st.error(f"ä¿å­˜å›¾ç‰‡å¤±è´¥: {str(e)}")
            return
    
    # å¦‚æœæ²¡æœ‰æ–‡æœ¬ä½†æœ‰å›¾ç‰‡ï¼Œå¡«å……é»˜è®¤æ–‡æœ¬
    if image_path and (text is None or not text.strip()):
        text = "åˆ†äº«äº†ä¸€å¼ å›¾ç‰‡"
    
    # å†™å…¥èŠå¤©è®°å½•
    with open("chat_messages/chat_history.txt", "a", encoding="utf-8") as f:
        f.write(f"{timestamp}|{sender}|{text or ''}|{image_path or ''}\n")

def load_messages():
    """åŠ è½½æ‰€æœ‰èŠå¤©æ¶ˆæ¯"""
    messages = []
    if os.path.exists("chat_messages/chat_history.txt"):
        with open("chat_messages/chat_history.txt", "r", encoding="utf-8") as f:
            for line in f.readlines():
                parts = line.strip().split("|")
                if len(parts) == 4:
                    timestamp, sender, text, image_path = parts
                    messages.append({
                        "time": datetime.strptime(timestamp, "%Y%m%d_%H%M%S"),
                        "sender": sender,
                        "text": text,
                        "image": image_path if image_path != "" else None
                    })
    return messages

def display_messages():
    """æ˜¾ç¤ºæ‰€æœ‰èŠå¤©æ¶ˆæ¯ï¼ˆä¸­æ–‡ç•Œé¢ï¼‰"""
    messages = load_messages()
    messages.sort(key=lambda x: x["time"])
    
    # åˆå§‹åŒ–é€‰ä¸­å†…å®¹ç›¸å…³çŠ¶æ€
    if "selected_items" not in st.session_state:
        st.session_state.selected_items = []  # å­˜å‚¨é€‰ä¸­å†…å®¹çš„å­—å…¸åˆ—è¡¨
    if "item_ids" not in st.session_state:
        st.session_state.item_ids = []
    
    for idx, msg in enumerate(messages):
        # åˆ¤æ–­æ˜¯å¦ä¸ºå¯¹æ–¹å‘é€çš„æ¶ˆæ¯
        is_other_user = msg["sender"] != st.session_state.current_user
        
        # æ¶ˆæ¯æ ·å¼è®¾ç½®
        message_align = "left" if is_other_user else "right"
        message_color = "lightgray" if is_other_user else "lightblue"
        
        # æ˜¾ç¤ºæ¶ˆæ¯
        st.markdown(f"""
        <div style="text-align: {message_align}; margin: 5px;">
            <div style="display: inline-block; background-color: {message_color}; 
                        padding: 8px 12px; border-radius: 12px; max-width: 70%;">
                <div style="font-size: 0.8em; color: gray;">{msg['sender']}</div>
                {msg['text'] if msg['text'] else ''}
            </div>
            <div style="font-size: 0.7em; color: gray; margin-top: 2px;">
                {msg['time'].strftime("%H:%M")}
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        # æ˜¾ç¤ºå›¾ç‰‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if msg["image"] and os.path.exists(msg["image"]):
            try:
                image = Image.open(msg["image"])
                st.image(image, width=200)
                
                # å›¾ç‰‡ä¸‹æ–¹æ·»åŠ æ“ä½œæŒ‰é’®
                col1, col2, col3 = st.columns([1, 1, 1])  # æ”¹ä¸º3åˆ—
                with col1:
                    # åªä¸ºå¯¹æ–¹å‘é€çš„å›¾ç‰‡æ·»åŠ é€‰æ‹©æ¡†
                    if is_other_user:
                        image_id = f"image_{idx}_{msg['time'].timestamp()}"
                        selected = st.checkbox(
                            "åˆ†æå›¾ç‰‡æƒ…æ„Ÿ",
                            key=f"select_{image_id}",
                            help="å‹¾é€‰ä»¥åˆ†ææ­¤å›¾ç‰‡æƒ…æ„Ÿ"
                        )
                        
                        if selected:
                            if image_id not in st.session_state.item_ids:
                                st.session_state.selected_items.append({
                                    "type": "image",
                                    "content": image,
                                    "text": ""  # å›¾ç‰‡æ²¡æœ‰æ–‡æœ¬å†…å®¹
                                })
                                st.session_state.item_ids.append(image_id)
                        else:
                            if image_id in st.session_state.item_ids:
                                index = st.session_state.item_ids.index(image_id)
                                st.session_state.selected_items.pop(index)
                                st.session_state.item_ids.pop(index)
                
                with col2:
                    # æ·»åŠ OCRæŒ‰é’®ï¼ˆæ‰€æœ‰å›¾ç‰‡éƒ½å¯OCRï¼‰
                    if st.button("è¯†åˆ«å›¾ä¸­æ–‡å­—", key=f"ocr_{idx}_{msg['time'].timestamp()}"):
                        ocr_text = ocr_extract_text(image)
                        st.session_state.current_ocr_text = ocr_text
                        st.session_state.show_ocr_dialog = True
                        st.rerun()
                
                with col3:  # æ–°å¢å›¾ç‰‡æè¿°æŒ‰é’®
                    if st.button("ç”Ÿæˆå›¾ç‰‡æè¿°", key=f"desc_{idx}_{msg['time'].timestamp()}"):
                        description = generate_image_description(image)
                        if description:
                            st.session_state.current_description = description
                            st.session_state.show_description_dialog = True
                            st.rerun()
                        
            except Exception as e:
                st.error(f"åŠ è½½å›¾ç‰‡å¤±è´¥: {str(e)}")
        
        # åªä¸ºå¯¹æ–¹å‘é€çš„æ–‡æœ¬æ¶ˆæ¯æ·»åŠ é€‰æ‹©æ¡†
        if is_other_user and msg["text"] and msg["text"].strip():
            text_id = f"text_{idx}_{msg['time'].timestamp()}"
            selected = st.checkbox(
                "é€‰æ‹©æ­¤æ¶ˆæ¯", 
                key=f"select_{text_id}",
                help="å‹¾é€‰ä»¥åˆ†ææ­¤æ¶ˆæ¯æƒ…æ„Ÿ"
            )
            
            if selected:
                if text_id not in st.session_state.item_ids:
                    st.session_state.selected_items.append({
                        "type": "text",
                        "content": msg["text"],
                        "image": None  # æ–‡æœ¬æ²¡æœ‰å›¾ç‰‡
                    })
                    st.session_state.item_ids.append(text_id)
            else:
                if text_id in st.session_state.item_ids:
                    index = st.session_state.item_ids.index(text_id)
                    st.session_state.selected_items.pop(index)
                    st.session_state.item_ids.pop(index)
    
    # æ·»åŠ åˆ†ææŒ‰é’®ï¼ˆä¸­æ–‡ç•Œé¢ï¼‰
    if st.session_state.selected_items:
        col1, col2 = st.columns([1, 3])
        with col1:
            if st.button("åˆ†æé€‰ä¸­å†…å®¹", help="åˆ†æå·²é€‰å†…å®¹å’Œå›¾ç‰‡çš„æƒ…æ„Ÿè¶‹åŠ¿"):
                st.session_state.show_sentiment_analysis = True
                st.rerun()
        with col2:
            if st.button("æ¸…é™¤é€‰æ‹©", help="æ¸…é™¤æ‰€æœ‰å·²é€‰å†…å®¹"):
                st.session_state.selected_items = []
                st.session_state.item_ids = []
                st.rerun()



def main():
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ï¼ˆæ–°å¢é…å›¾ç›¸å…³çŠ¶æ€ï¼‰
    if "current_user" not in st.session_state:
        st.session_state.current_user = "ç”¨æˆ·A"
    if "show_gugu_dialog" not in st.session_state:
        st.session_state.show_gugu_dialog = False
    if "gugu_results" not in st.session_state:
        st.session_state.gugu_results = []
    if "show_translation_dialog" not in st.session_state:
        st.session_state.show_translation_dialog = False
    if "show_description_dialog" not in st.session_state:
        st.session_state.show_description_dialog = False
    if "current_description" not in st.session_state:
        st.session_state.current_description = None
    if "show_bleu_dialog" not in st.session_state:
        st.session_state.show_bleu_dialog = False
    if "show_sentiment_analysis" not in st.session_state:
        st.session_state.show_sentiment_analysis = False
    if "selected_texts" not in st.session_state:
        st.session_state.selected_texts = []
    if "text_ids" not in st.session_state:
        st.session_state.text_ids = []
    if "last_message_type" not in st.session_state:
        st.session_state.lastmessage_type = None
    if "show_ocr_dialog" not in st.session_state:
        st.session_state.show_ocr_dialog = False
    if "current_ocr_text" not in st.session_state:
        st.session_state.current_ocr_text = ""
    # æ–°å¢é…å›¾ç›¸å…³çŠ¶æ€
    if "show_meme_dialog" not in st.session_state:
        st.session_state.show_meme_dialog = False
    if "preview_meme" not in st.session_state:
        st.session_state.preview_meme = None
    if "selected_meme_idx" not in st.session_state:
        st.session_state.selected_meme_idx = 0
    if "meme_match_mode" not in st.session_state:
        st.session_state.meme_match_mode = "å†…å®¹åŒ¹é…(BLEU)"

    # åŠ è½½åŒ¹é…å™¨
    matcher = load_matcher()

    # é¡µé¢æ ‡é¢˜
    st.title("ğŸ’¬ åŒäººèŠå¤©å®¤")
    
    # ç”¨æˆ·é€‰æ‹©
    st.sidebar.title("ç”¨æˆ·è®¾ç½®")
    user = st.sidebar.radio("å½“å‰ç”¨æˆ·", ["ç”¨æˆ·A", "ç”¨æˆ·B"])
    st.session_state.current_user = user
    
    # èŠå¤©è®°å½•æ˜¾ç¤ºåŒºåŸŸ
    st.subheader("èŠå¤©è®°å½•")
    display_messages()
    
    # æ¶ˆæ¯è¾“å…¥åŒºåŸŸ - ä½¿ç”¨ç‹¬ç«‹çš„form
    with st.form(key="message_form"):
        st.subheader("å‘é€æ¶ˆæ¯")
        
        text_input = st.text_area("è¾“å…¥æ¶ˆæ¯", height=100, key="text_input")
        image_upload = st.file_uploader("ä¸Šä¼ å›¾ç‰‡", type=["png", "jpg", "jpeg"], key="image_upload")
        
        # æŒ‰é’®å¸ƒå±€ï¼ˆä»4åˆ—æ‰©å±•ä¸º5åˆ—ï¼‰
        col1, col2, col3, col4, col5 = st.columns([3, 1, 1, 1, 1])
        with col1:
            submitted = st.form_submit_button("å‘é€")
        with col2:
            gugu_speak = st.form_submit_button("GuGuè¯´è¯")
        with col3:
            gugu_translate = st.form_submit_button("GuGuç¿»è¯‘")
        with col4:
            bleu_match = st.form_submit_button("BLEUåŒ¹é…")
        with col5:
            meme_btn = st.form_submit_button("GuGué…å›¾")  # æ–°å¢æŒ‰é’®
            
            
        if submitted:
            img_obj = None
            if image_upload:
                img_obj = Image.open(image_upload)
            
            # è‡ªåŠ¨å¤„ç†æ–‡æœ¬å’Œå›¾ç‰‡çš„ç»„åˆ
            save_message(
                st.session_state.current_user,
                text=text_input if text_input and text_input.strip() else None,
                image=img_obj
            )
            st.rerun()
        
        if gugu_speak and text_input:
            results = find_similar_sentences(text_input, top_n=3)
            st.session_state.gugu_results = results
            st.session_state.show_gugu_dialog = True
            st.rerun()
        
        if gugu_translate and text_input:
            st.session_state.show_translation_dialog = True
            st.rerun()
            
        if bleu_match and text_input:
            results = find_similar_by_bleu(text_input, top_n=3)
            st.session_state.gugu_results = results
            st.session_state.show_bleu_dialog = True
            st.rerun()
        
        if meme_btn and text_input:  # æ–°å¢é…å›¾æŒ‰é’®å¤„ç†
            st.session_state.show_meme_dialog = True
            st.rerun()
    
    # BLEUåŒ¹é…å¯¹è¯æ¡†
    if st.session_state.show_bleu_dialog:
        with st.container():
            st.subheader("BLEUåŒ¹é…ç»“æœ")
            
            selected_index = st.radio(
                "åŒ¹é…ç»“æœ:",
                options=range(len(st.session_state.gugu_results)),
                format_func=lambda i: f"{st.session_state.gugu_results[i]['text']} (BLEUåˆ†æ•°: {st.session_state.gugu_results[i]['score']:.4f})"
            )
            
            col1, col2 = st.columns([1, 2])
            with col1:
                if st.button("ç¡®è®¤å‘é€", key="bleu_confirm"):
                    save_message(st.session_state.current_user,
                               text=st.session_state.gugu_results[selected_index]["text"])
                    st.session_state.show_bleu_dialog = False
                    st.rerun()
            with col2:
                if st.button("å–æ¶ˆ", key="bleu_cancel"):
                    st.session_state.show_bleu_dialog = False
                    st.rerun()

    # GuGué…å›¾å¯¹è¯æ¡†ï¼ˆæ–°å¢ï¼‰
    if st.session_state.show_meme_dialog:
        with st.container():
            st.subheader("GuGuè¡¨æƒ…åŒ…åŒ¹é…")
            
            # åŒ¹é…æ¨¡å¼é€‰æ‹©
            st.session_state.meme_match_mode = st.radio(
                "é€‰æ‹©åŒ¹é…æ¨¡å¼",
                ["å†…å®¹åŒ¹é…(BLEU)", "æƒ…æ„ŸåŒ¹é…"],
                horizontal=True,
                index=0 if st.session_state.meme_match_mode == "å†…å®¹åŒ¹é…(BLEU)" else 1
            )
            
            # è·å–åŒ¹é…ç»“æœ
            if st.session_state.meme_match_mode == "å†…å®¹åŒ¹é…(BLEU)":
                results = matcher.bleu_match(st.session_state.text_input, top_n=5)
            else:
                results = matcher.emotion_match(st.session_state.text_input, top_n=5)
            
            # æ˜¾ç¤ºåŒ¹é…ç»“æœ
            st.session_state.selected_meme_idx = st.selectbox(
                "é€‰æ‹©è¦å‘é€çš„è¡¨æƒ…åŒ…",
                range(len(results)),
                format_func=lambda i: f"{results[i]['filename']} (åˆ†æ•°: {results[i].get('score', results[i].get('similarity', 0)):.2f})"
            )
            
        # æ“ä½œæŒ‰é’®
        col1, col2, col3 = st.columns([1, 1, 2])
        with col1:
            if st.button("é¢„è§ˆ"):
                st.session_state.preview_meme = f"./code/emo/{results[st.session_state.selected_meme_idx]['filename']}"
        with col2:
            if st.button("ç¡®è®¤å‘é€"):
                # å…³é”®ä¿®æ”¹ç‚¹ï¼šå°†image_pathæ”¹ä¸ºimage
                meme_path = f"./code/emo/{results[st.session_state.selected_meme_idx]['filename']}"
                save_message(
                    st.session_state.current_user,
                    text=st.session_state.text_input,
                    image=meme_path  # ä½¿ç”¨imageå‚æ•°ä¼ é€’è·¯å¾„
                )
                st.session_state.show_meme_dialog = False
                st.session_state.preview_meme = None
                st.rerun()
        with col3:
            if st.button("å–æ¶ˆ"):
                st.session_state.show_meme_dialog = False
                st.session_state.preview_meme = None
                st.rerun()
        
        # é¢„è§ˆåŒºåŸŸ
        if st.session_state.preview_meme:
            st.image(st.session_state.preview_meme, width=300)
            

    # OCRç»“æœå¯¹è¯æ¡†ï¼ˆç®€åŒ–ç‰ˆï¼‰
    if st.session_state.show_ocr_dialog:
        with st.container():
            st.subheader("æ–‡å­—è¯†åˆ«ç»“æœ")
            
            # æ˜¾ç¤ºè¯†åˆ«æ–‡æœ¬ï¼ˆå¯å¤åˆ¶ï¼‰
            st.text_area(
                "è¯†åˆ«åˆ°çš„æ–‡å­—",
                st.session_state.current_ocr_text,
                height=200,
                key="ocr_result_area"
            )
            
            if st.button("å…³é—­", key="close_ocr"):
                st.session_state.show_ocr_dialog = False
                st.rerun()

    # GuGuè¯´è¯ç¡®è®¤å¯¹è¯æ¡†
    if st.session_state.show_gugu_dialog:
        with st.container():
            st.subheader("è¯·é€‰æ‹©è¦å‘é€çš„æ¶ˆæ¯")
            
            selected_index = st.radio(
                "GuGuçŒœä½ æƒ³è¯´:",
                options=range(len(st.session_state.gugu_results)),
                format_func=lambda i: f"{st.session_state.gugu_results[i]['text']} (ç›¸ä¼¼åº¦: {st.session_state.gugu_results[i]['similarity']:.2f})"
            )
            
            col1, col2 = st.columns([1, 2])
            with col1:
                if st.button("ç¡®è®¤å‘é€"):
                    # ä¿®æ”¹è¿™é‡Œï¼šä½¿ç”¨å½“å‰ç”¨æˆ·èº«ä»½è€Œä¸æ˜¯"GuGu"
                    save_message(st.session_state.current_user, 
                               text=st.session_state.gugu_results[selected_index]["text"])
                    st.session_state.show_gugu_dialog = False
                    st.rerun()
            with col2:
                if st.button("å–æ¶ˆ"):
                    st.session_state.show_gugu_dialog = False
                    st.rerun()
    
    # ç¿»è¯‘å¯¹è¯æ¡†
    if st.session_state.show_translation_dialog:
        with st.container():
            st.subheader("ç¿»è¯‘ç»“æœ")
            
            try:
                translated_text = translate_text(st.session_state.text_input)
                st.markdown(f"**åŸæ–‡**: {st.session_state.text_input}")
                st.markdown(f"**ç¿»è¯‘**: {translated_text}")
                
                col1, col2 = st.columns([1, 2])
                with col1:
                    if st.button("å‘é€ç¿»è¯‘ç»“æœ"):
                        # ä¿®æ”¹è¿™é‡Œï¼šä½¿ç”¨å½“å‰ç”¨æˆ·èº«ä»½è€Œä¸æ˜¯"GuGu"
                        save_message(st.session_state.current_user,
                                   text=f"ç¿»è¯‘: {translated_text}")
                        st.session_state.show_translation_dialog = False
                        st.rerun()
                with col2:
                    if st.button("å–æ¶ˆç¿»è¯‘"):
                        st.session_state.show_translation_dialog = False
                        st.rerun()
            except Exception as e:
                st.error(f"ç¿»è¯‘å‡ºé”™: {str(e)}")
                if st.button("å…³é—­"):
                    st.session_state.show_translation_dialog = False
                    st.rerun()

    # å›¾ç‰‡æè¿°å¯¹è¯æ¡†
    if st.session_state.show_description_dialog and st.session_state.current_description:
        with st.container():
            st.subheader("GuGuå›¾ç‰‡æè¿°")
            
            desc = st.session_state.current_description
            st.markdown(f"**è‹±æ–‡æè¿°**: {desc['english']}")
            st.markdown(f"**ä¸­æ–‡ç¿»è¯‘**: {desc['chinese']}")
            
            col1, col2 = st.columns([1, 2])
            with col1:
                if st.button("å‘é€æè¿°"):
                    # ä¿®æ”¹è¿™é‡Œï¼šä½¿ç”¨å½“å‰ç”¨æˆ·èº«ä»½è€Œä¸æ˜¯"GuGu"
                    save_message(st.session_state.current_user,
                               text=f"å›¾ç‰‡æè¿°: {desc['chinese']}")
                    st.session_state.show_description_dialog = False
                    st.rerun()
            with col2:
                if st.button("å–æ¶ˆ"):
                    st.session_state.show_description_dialog = False
                    st.rerun()

    # æƒ…æ„Ÿåˆ†æå¯¹è¯æ¡†ï¼ˆç§»åŠ¨åˆ°è¿™é‡Œï¼ï¼‰
    if st.session_state.show_sentiment_analysis and st.session_state.selected_items:
        with st.container():
            st.subheader("å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æç»“æœ")
            
            # åˆ†æé€‰ä¸­çš„å†…å®¹
            negative_logits = []
            positive_logits = []
            descriptions = []
            
            for item in st.session_state.selected_items:
                if item["type"] == "text":
                    # çº¯æ–‡æœ¬åˆ†æ
                    neg, pos = analyze_multimodal_sentiment(text=item["content"])
                    descriptions.append(f"æ–‡æœ¬: {item['content']}")
                else:
                    # å›¾ç‰‡åˆ†æ - ç”Ÿæˆæè¿°
                    desc = generate_image_description(item["content"])
                    if desc:
                        # ä½¿ç”¨å›¾ç‰‡æè¿°å’Œå›¾ç‰‡æœ¬èº«è¿›è¡Œå¤šæ¨¡æ€åˆ†æ
                        neg, pos = analyze_multimodal_sentiment(
                            text=desc["english"], 
                            image=item["content"]
                        )
                        descriptions.append(f"å›¾ç‰‡æè¿°: {desc['chinese']}")
                    else:
                        # ä»…ä½¿ç”¨å›¾ç‰‡åˆ†æ
                        neg, pos = analyze_multimodal_sentiment(image=item["content"])
                        descriptions.append("å›¾ç‰‡(æ— æ³•ç”Ÿæˆæè¿°)")
                
                negative_logits.append(neg)
                positive_logits.append(pos)
            
            # ç»˜åˆ¶æ›²çº¿å›¾ï¼ˆå›¾åƒå†…éƒ¨è‹±æ–‡ï¼‰
            st.write("æƒ…æ„Ÿè¶‹åŠ¿å›¾ï¼š")
            fig = plot_sentiment_curve(
                range(len(st.session_state.selected_items)),
                negative_logits,
                positive_logits
            )
            st.pyplot(fig)
            
            # æ˜¾ç¤ºè¯¦ç»†æ•°æ®ï¼ˆä¸­æ–‡ç•Œé¢ï¼‰
            st.write("è¯¦ç»†åˆ†æç»“æœï¼š")
            for i, (desc, neg, pos) in enumerate(zip(descriptions, negative_logits, positive_logits)):
                st.markdown(f"""
                **å†…å®¹ {i+1}**:  
                {desc}  
                è´Ÿé¢æƒ…æ„Ÿå¼ºåº¦: {neg:.2f}  
                æ­£é¢æƒ…æ„Ÿå¼ºåº¦: {pos:.2f}
                """)
            
            if st.button("è¿”å›", help="è¿”å›èŠå¤©ç•Œé¢"):
                st.session_state.show_sentiment_analysis = False
                st.rerun()


if __name__ == "__main__":
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["STREAMLIT_SERVER_ENABLE_FILE_WATCHER"] = "false"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    # å¿½ç•¥ç‰¹å®šè­¦å‘Š
    import warnings
    warnings.filterwarnings("ignore", message="Using a slow image processor")
    
    main()